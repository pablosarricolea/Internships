{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROEBLEMs:\n",
    "\n",
    "a problem with the shuffling of the data: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "give weights to classes: #https://www.tensorflow.org/tutorials/structured_data/imbalanced_data --> it'll depend on the magnitude of the loss functions\n",
    "validation rn --> change loading images train, we've done it before\n",
    "dealing with class imbalance --> file:///C:/Users/pablo/Downloads/Survey_on_deep_learning_with_class_imbalance.pdf\n",
    "\n",
    "TODOs:\n",
    "\n",
    "test my model with oxford --> https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet /// https://www.kaggle.com/spidy20/image-segmentation-using-unet-tensorflow\n",
    "check the activation of layers --> https://awjuliani.medium.com/visualizing-neural-network-layer-activation-tensorflow-tutorial-d45f8bf7bbc4\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0', '/device:XLA_GPU:1', '/device:GPU:0', '/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "#Run from server\n",
    "\n",
    "gpu4 = \"GPU-71b4cdfc-e381-0b98-9b24-4fc06284b496\" \n",
    "gpu5 = \"GPU-99d0769a-9f86-4800-a40e-2320dddcf5d1\" \n",
    "gpu6 = \"GPU-7423cfb5-cff4-ec4d-7e96-ea6e1591d56f\"\n",
    "gpu7 = \"GPU-c0a8738f-6dd0-1b78-c38f-4969fd3886a8\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu6 + \",\" + gpu5\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, Lambda, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Dropout, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 240\n",
    "IMG_WIDTH = 240\n",
    "IMG_DEPTH = 155\n",
    "N_IMG = 369\n",
    "length_file = IMG_DEPTH * N_IMG\n",
    "h5py_file_name = 'training.hdf5'\n",
    "smooth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and create data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImgDir = os.path.join(\"..\",\"data\",\"MICCAI_BraTS2020_TrainingData\")\n",
    "features_path = list()\n",
    "labels_path = list()\n",
    "count = 0\n",
    "lim = 10\n",
    "\n",
    "for folder in os.listdir(ImgDir):\n",
    "    count +=1\n",
    "    if 'Training' in folder:\n",
    "        new_dir = os.path.join(ImgDir,folder)\n",
    "        data = os.listdir(new_dir)\n",
    "        for files in data:\n",
    "            if 'flair' in files:\n",
    "                features_path.append(os.path.join(new_dir, files))\n",
    "            if 'seg' in files:\n",
    "                labels_path.append(os.path.join(new_dir, files))\n",
    "\n",
    "print(len(features_path))\n",
    "print(len(labels_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_conc_features = np.zeros((len(features_path),IMG_HEIGHT,IMG_WIDTH,IMG_DEPTH))\n",
    "path = features_path\n",
    "\n",
    "for file, i in zip(path,range(len(path))):\n",
    "    img = nib.load(file)\n",
    "    imgarr = img.get_fdata()\n",
    "    img_conc_features[i,:,:,:] = imgarr\n",
    "    \n",
    "img_conc_features = np.concatenate(img_conc_features,axis=2)\n",
    "print(np.shape(img_conc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_conc_labels = np.zeros((len(labels_path),IMG_HEIGHT,IMG_WIDTH,IMG_DEPTH))\n",
    "path = labels_path\n",
    "\n",
    "for file, i in zip(path,range(len(path))):\n",
    "    img = nib.load(file)\n",
    "    imgarr = img.get_fdata()\n",
    "    img_conc_labels[i,:,:,:] = imgarr\n",
    "    \n",
    "img_conc_labels = np.concatenate(img_conc_labels,axis=2)\n",
    "print(np.shape(img_conc_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join('..','data',h5py_file_name), 'a') as f:\n",
    "    f.create_dataset(\"features\", data=img_conc_features, compression=\"gzip\")\n",
    "    f.create_dataset(\"labels\", data=img_conc_labels, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_store_Brats = h5py.File(os.path.join('..','data',h5py_file_name), \"r\")\n",
    "test_img = hdf5_store_Brats[\"features\"][:, :, 100]\n",
    "plt.imshow(test_img, cmap ='gray')\n",
    "hdf5_store_Brats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_store_Brats = h5py.File(os.path.join('..','data',h5py_file_name), \"r\")\n",
    "test_img = hdf5_store_Brats[\"labels\"][:, :, 100]\n",
    "plt.imshow(test_img, cmap ='gray')\n",
    "hdf5_store_Brats.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = X_nib.get_fdata()\n",
    "# labels = y_nib.get_fdata()\n",
    "\n",
    "with h5py.File(os.path.join('..','data',h5py_file_name), \"r\") as f:\n",
    "    images_train = f[\"features\"][()]\n",
    "    labels_train = f[\"labels\"][()]\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/using-custom-building-blocks-in-tensorflow-2-0-550b88eb7aa2\n",
    "\n",
    "class My_Custom_Generator(tf.keras.utils.Sequence):\n",
    "  \n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = True\n",
    "        \n",
    "#     def on_epoch_end(self):\n",
    "#         self.indexes = np.arange(len(self.list_IDs))\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.images[:,:,idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[:,:,idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        #240,240,32\n",
    "        batch_x /= 255.\n",
    "        batch_y /= 255.\n",
    "\n",
    "        train_image = np.zeros((batch_size,IMG_HEIGHT,IMG_WIDTH,3))\n",
    "        train_label = np.zeros((batch_size,IMG_HEIGHT,IMG_WIDTH,3))\n",
    "        \n",
    "        #for i in range(0, len(batch_x)):\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            X_new = np.zeros((240, 240 ,3), np.float32)\n",
    "            y_new = np.zeros((240, 240 ,3), np.float32)\n",
    "\n",
    "            X_new[:,:,0] = batch_x[:,:,i]\n",
    "            X_new[:,:,1] = batch_x[:,:,i]\n",
    "            X_new[:,:,2] = batch_x[:,:,i]\n",
    "\n",
    "            y_new[:,:,0] = batch_y[:,:,i]\n",
    "            y_new[:,:,1] = batch_y[:,:,i]\n",
    "            y_new[:,:,2] = batch_y[:,:,i]\n",
    "\n",
    "            train_image[i,:,:,:] = X_new\n",
    "            train_label[i,:,:,:] = y_new\n",
    "\n",
    "            #img_path = batch_x[i]\n",
    "            #label = batch_y[i]\n",
    "            #  # read method takes image path and label and returns corresponding matrices\n",
    "            #image, label_matrix = read(img_path, label)\n",
    "            #train_image.append(image)\n",
    "            #train_label.append(label_matrix)\n",
    "            \n",
    "        #test channel creation done succesfully\n",
    "        \n",
    "        return np.array(train_image), np.array(train_label)\n",
    "\n",
    "my_training_batch_generator = My_Custom_Generator(images_train, labels_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class printbatch(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    #def on_epoch_end(self, epoch, logs=None):\n",
    "    #    keys = list(logs.keys())\n",
    "    #    \n",
    "    #    print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "pb = printbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return (1-dice_coef(y_true, y_pred))\n",
    "\n",
    "def get_model():\n",
    "    in1 = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(in1)\n",
    "    conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)\n",
    "    conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool3)\n",
    "    conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)\n",
    "\n",
    "    up1 = concatenate([UpSampling2D((2, 2))(conv4), conv3], axis=-1)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up1)\n",
    "    conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv5)\n",
    "    \n",
    "    up2 = concatenate([UpSampling2D((2, 2))(conv5), conv2], axis=-1)\n",
    "    conv6 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up2)\n",
    "    conv6 = Dropout(0.2)(conv6)\n",
    "    conv6 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n",
    "\n",
    "    up2 = concatenate([UpSampling2D((2, 2))(conv6), conv1], axis=-1)\n",
    "    conv7 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up2)\n",
    "    conv7 = Dropout(0.2)(conv7)\n",
    "    conv7 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n",
    "    segmentation = Conv2D(3, (1, 1), activation='sigmoid', name='seg')(conv7)\n",
    "\n",
    "    model = Model(inputs=[in1], outputs=[segmentation])   \n",
    "\n",
    "    #model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000000199), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 240, 240, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 240, 240, 16) 448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 240, 240, 16) 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 240, 240, 16) 2320        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 120, 120, 16) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 120, 120, 32) 4640        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 120, 120, 32) 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 120, 120, 32) 9248        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 60, 60, 32)   0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 60, 60, 64)   18496       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 60, 60, 64)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 60, 60, 64)   36928       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 30, 30, 64)   0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 30, 30, 64)   36928       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 30, 30, 64)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 30, 30, 64)   36928       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 60, 60, 64)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 60, 60, 128)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 60, 60, 32)   36896       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 60, 60, 32)   0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 60, 60, 32)   9248        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 120, 120, 32) 0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 120, 120, 64) 0           up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 120, 120, 32) 18464       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 120, 120, 32) 0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 120, 120, 32) 9248        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 240, 240, 32) 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 240, 240, 48) 0           up_sampling2d_8[0][0]            \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 240, 240, 16) 6928        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 240, 240, 16) 0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 240, 240, 16) 2320        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "seg (Conv2D)                    (None, 240, 240, 3)  51          conv2d_41[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 229,091\n",
      "Trainable params: 229,091\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2=get_model()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6169e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6169e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6169e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6169e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6170e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6170e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 685ms/step - loss: 1.0000 - dice_coef: 3.6170e-07\n",
      "Epoch 2/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6172e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6173e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6173e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6173e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6174e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6174e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 623ms/step - loss: 1.0000 - dice_coef: 3.6174e-07\n",
      "Epoch 3/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6177e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6178e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6178e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6178e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6179e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6179e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 601ms/step - loss: 1.0000 - dice_coef: 3.6180e-07\n",
      "Epoch 4/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6183e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6183e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6184e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6184e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6185e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6185e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 661ms/step - loss: 1.0000 - dice_coef: 3.6185e-07\n",
      "Epoch 5/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6189e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6189e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6190e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6190e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6191e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6191e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 682ms/step - loss: 1.0000 - dice_coef: 3.6192e-07\n",
      "Epoch 6/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6196e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6196e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6197e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6197e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6197e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6198e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 520ms/step - loss: 1.0000 - dice_coef: 3.6198e-07\n",
      "Epoch 7/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6202e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6203e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6203e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6204e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6204e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6205e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 607ms/step - loss: 1.0000 - dice_coef: 3.6205e-07\n",
      "Epoch 8/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6210e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6210e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6211e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6211e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6212e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6212e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 670ms/step - loss: 1.0000 - dice_coef: 3.6213e-07\n",
      "Epoch 9/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6217e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6217e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6218e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6218e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6219e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6220e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 531ms/step - loss: 1.0000 - dice_coef: 3.6220e-07\n",
      "Epoch 10/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6224e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6225e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6226e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6226e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6227e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6227e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 3s 395ms/step - loss: 1.0000 - dice_coef: 3.6228e-07\n",
      "Epoch 11/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6232e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6233e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6233e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6234e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6234e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6235e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 717ms/step - loss: 1.0000 - dice_coef: 3.6235e-07\n",
      "Epoch 12/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6240e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6241e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6241e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6242e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6242e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6243e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 551ms/step - loss: 1.0000 - dice_coef: 3.6243e-07\n",
      "Epoch 13/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6248e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6249e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6249e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6250e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6250e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6251e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 541ms/step - loss: 1.0000 - dice_coef: 3.6251e-07\n",
      "Epoch 14/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6256e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6257e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6257e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6258e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6258e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6259e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 6s 788ms/step - loss: 1.0000 - dice_coef: 3.6260e-07\n",
      "Epoch 15/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6264e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6265e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6265e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6266e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6267e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6267e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 4s 524ms/step - loss: 1.0000 - dice_coef: 3.6268e-07\n",
      "Epoch 16/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6273e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6273e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6274e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6274e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6275e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6276e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 750ms/step - loss: 1.0000 - dice_coef: 3.6276e-07\n",
      "Epoch 17/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6281e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6282e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6282e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6283e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6283e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6284e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 656ms/step - loss: 1.0000 - dice_coef: 3.6285e-07\n",
      "Epoch 18/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6290e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6290e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6291e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6291e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6292e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6293e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 3s 479ms/step - loss: 1.0000 - dice_coef: 3.6293e-07\n",
      "Epoch 19/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6298e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6299e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6299e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6300e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6301e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6301e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 700ms/step - loss: 1.0000 - dice_coef: 3.6302e-07\n",
      "Epoch 20/20\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'dice_coef']\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6307e-07...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'dice_coef']\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6308e-07...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'dice_coef']\n",
      "3/7 [===========>..................] - ETA: 2s - loss: 1.0000 - dice_coef: 3.6308e-07...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'dice_coef']\n",
      "4/7 [================>.............] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6309e-07...Training: start of batch 4; got log keys: []\n",
      "...Training: end of batch 4; got log keys: ['loss', 'dice_coef']\n",
      "5/7 [====================>.........] - ETA: 1s - loss: 1.0000 - dice_coef: 3.6309e-07...Training: start of batch 5; got log keys: []\n",
      "...Training: end of batch 5; got log keys: ['loss', 'dice_coef']\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.0000 - dice_coef: 3.6310e-07...Training: start of batch 6; got log keys: []\n",
      "...Training: end of batch 6; got log keys: ['loss', 'dice_coef']\n",
      "7/7 [==============================] - 5s 663ms/step - loss: 1.0000 - dice_coef: 3.6311e-07\n"
     ]
    }
   ],
   "source": [
    "# modify the fit_generator call to include the callback pb\n",
    "training_history = model2.fit(x = my_training_batch_generator, steps_per_epoch = int(len(images_train) // batch_size), epochs = 20, verbose=1, callbacks=[pb], validation_data=None, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b7e3d06581ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-b104b3b3f7e4>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "img_flair = sitk.ReadImage(os.path.join(\"..\", \"data\",\"MICCAI_BraTS2020_TrainingData\",\"BraTS20_Training_001\",\"BraTS20_Training_001_flair.nii.gz\"))\n",
    "nda_flair = sitk.GetArrayViewFromImage(img_flair)\n",
    "print(np.shape(nda_flair))\n",
    "\n",
    "for i in range(len(nda_flair[:,1,1])):\n",
    "\n",
    "    X_new = np.zeros((240, 240 ,3), np.float32)\n",
    "\n",
    "    X_new[:,:,0] = batch_x[:,:,i]\n",
    "    X_new[:,:,1] = batch_x[:,:,i]\n",
    "    X_new[:,:,2] = batch_x[:,:,i]\n",
    "\n",
    "    train_image.append(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    \n",
    "    data_length = len(X_nib.get_fdata()[1,1,:])\n",
    "    chunks = np.floor(data_length / batch_size)\n",
    "    print('A')\n",
    "    X = X_nib.get_fdata()\n",
    "    y = y_nib.get_fdata()\n",
    "    print('B')\n",
    "    X /= 255\n",
    "    y /= 255\n",
    "    print('C')\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for i in range(int(chunks)):\n",
    "            X_yield = []\n",
    "            y_yield = []\n",
    "\n",
    "            X_mod = X[:,:,i*32:(i+1)*32]\n",
    "            y_mod = y[:,:,i*32:(i+1)*32]\n",
    "            print(np.shape(X_mod))\n",
    "\n",
    "            for j in range(32):\n",
    "                X_new = np.zeros((240, 240 ,3), np.uint8)\n",
    "                X_new[:,:,0] = X_mod[:,:,j]\n",
    "                X_new[:,:,1] = X_mod[:,:,j]\n",
    "                X_new[:,:,2] = X_mod[:,:,j]\n",
    "\n",
    "                X_yield.append(X_new)\n",
    "                print(np.shape(X_yield))\n",
    "\n",
    "                y_new = np.zeros((240, 240 ,3), np.uint8)\n",
    "                y_new[:,:,0] = y_mod[:,:,j]\n",
    "                y_new[:,:,1] = y_mod[:,:,j]\n",
    "                y_new[:,:,2] = y_mod[:,:,j]\n",
    "\n",
    "                y_yield.append(y_new)\n",
    "\n",
    "            yield np.reshape(X_yield,(32,240,240,3)), np.reshape(y_yield,(32,240,240,3))\n",
    "        \n",
    "\n",
    "my_generator = generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_1",
   "language": "python",
   "name": "project_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
